{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/AI/blob/main/naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HclJutxS2bYj"
      },
      "source": [
        "# Naive Bayes\n",
        "\n",
        "The following is an explanation of Naive Bayes (8.7) in Ertel's Artificial Intelligence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pfU1f0FaWb8"
      },
      "source": [
        "## Naive Bayes class\n",
        "\n",
        "The code is adapted from [ML from Scratch Naive Bayes](https://github.com/python-engineer/MLfromscratch/blob/master/mlfromscratch/naivebayes.py) with the accompanying [video](https://www.youtube.com/watch?v=BqUmKsfSWho).\n",
        "\n",
        "$$ P(y|X) = \\large \\frac{P(X|y) \\cdot P(y)}{P(X)} $$\n",
        "\n",
        "- P(y|X): the posterior\n",
        "- P(X|y): likelihood - class conditional probability\n",
        "- P(X): evidence\n",
        "- P(y): prior probability\n",
        "\n",
        "You can calculate the posterior conditional probabilities for all classes and the one with the highest probability is the predicted class. In other words,\n",
        "\n",
        "$$ y = argmax_yP(y|X) $$\n",
        "\n",
        "But given that each class variable is independent of each other (which is why it is called the \"naive Bayes\"), it is turned into:\n",
        "\n",
        "$$ = argmax_y\\frac{P(x_1|y)P(x2|y)...P(x_n|y) \\cdot P(y)}{P(X)} $$\n",
        "\n",
        "The formula can be simplied further by dropping P(X) because it is a constant factor for all probabilities\n",
        "\n",
        "### Prior probabilities\n",
        "$ P(y) $: frequency of the class sample in the data\n",
        "### Class conditional probability\n",
        "For class conditional probabilities, the formula is the Gaussian distribution:\n",
        "\n",
        "$$ P(x_i|y) = \\frac{1}{\\sqrt {2\\pi\\sigma_y^2}} \\cdot exp(-\\frac{(x_i - \\mu_i)^2}{2\\sigma_y^2}) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQqCIQRKaHyM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NaiveBayes:\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        # calculate mean, var, and prior for each class\n",
        "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._priors =  np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            X_c = X[y==c]\n",
        "            self._mean[idx, :] = X_c.mean(axis=0)\n",
        "            self._var[idx, :] = X_c.var(axis=0)\n",
        "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        # calculate posterior probability for each class\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            prior = np.log(self._priors[idx])\n",
        "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
        "            posterior = prior + posterior\n",
        "            posteriors.append(posterior)\n",
        "            \n",
        "        # return class with highest posterior probability\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "            \n",
        "\n",
        "    def _pdf(self, class_idx, x):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = self._var[class_idx]\n",
        "        numerator = np.exp(- (x-mean)**2 / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        return numerator / denominator"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C0aHJYKbqDt"
      },
      "source": [
        "# Test\n",
        "Using the NaiveClass, we can run against the skiing example in 8.4. We can see that it correctly classified all examples except one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK4kVscObsCR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = np.array([\n",
        "      [6., 1.],\n",
        "      [7., 3.],\n",
        "      [8., 2.],    \n",
        "      [9., 0.],\n",
        "      [8., 4.],\n",
        "      [8., 6.],\n",
        "      [9., 2.],    \n",
        "      [9., 5.]\n",
        "    ])\n",
        "y = np.array([0, 0, 0, 0, 1, 1, 1, 1])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HAOxIyWcATh",
        "outputId": "b67f698d-64b8-461f-9f4e-c99697095008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj3YY2J8bs1b"
      },
      "source": [
        "nb = NaiveBayes()\n",
        "nb.fit(X, y)\n",
        "predictions = nb.predict(X)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAki7h5EcHZM",
        "outputId": "526c15dd-e1a4-479f-e389-d8988be6568f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WuQkfsMcJND"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}