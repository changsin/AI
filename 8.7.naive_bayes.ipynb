{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/AI/blob/main/8.7.naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HclJutxS2bYj"
      },
      "source": [
        "# Naive Bayes\n",
        "\n",
        "The following is an explanation of Naive Bayes (8.7) in Ertel's Artificial Intelligence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pfU1f0FaWb8"
      },
      "source": [
        "## Naive Bayes class\n",
        "\n",
        "The code is adapted from [ML from Scratch Naive Bayes](https://github.com/python-engineer/MLfromscratch/blob/master/mlfromscratch/naivebayes.py) with the accompanying [video](https://www.youtube.com/watch?v=BqUmKsfSWho).\n",
        "\n",
        "** Note that I fixed minor bugs in the calculation of the pdf below.\n",
        "\n",
        "$$ P(y|X) = \\large \\frac{P(X|y) \\cdot P(y)}{P(X)} $$\n",
        "\n",
        "- P(y|X): the posterior\n",
        "- P(X|y): likelihood - class conditional probability\n",
        "- P(X): evidence\n",
        "- P(y): prior probability\n",
        "\n",
        "You can calculate the posterior conditional probabilities for all classes and the one with the highest probability is the predicted class. In other words,\n",
        "\n",
        "$$ y = argmax_yP(y|X) $$\n",
        "\n",
        "But given that each class variable is independent of each other (which is why it is called the \"naive Bayes\"), it is turned into:\n",
        "\n",
        "$$ = argmax_y\\frac{P(x_1|y)P(x2|y)...P(x_n|y) \\cdot P(y)}{P(X)} $$\n",
        "\n",
        "The formula can be simplied further by dropping P(X) because it is a constant factor for all probabilities. \n",
        "\n",
        "$$ = argmax_y P(x_1|y)P(x2|y)...P(x_n|y) \\cdot P(y) $$\n",
        "\n",
        "The probabilities are all between zero and one so multiplying fractions will make the values approach to zero or in some cases becomes a zero probability. To avoid this problem, we want to use the log probabilities instead:\n",
        "\n",
        "\n",
        "$$ = argmax_y log(P(x_1|y)) + log(P(x2|y))...log(P(x_n|y)) + log(P(y)) $$\n",
        "\n",
        "So this is what we are going to use to code.\n",
        "\n",
        "\n",
        "### Prior probabilities\n",
        "$ P(y) $: frequency of the class sample in the data\n",
        "### Class conditional probability\n",
        "For class conditional probabilities, let's use the Gaussian distribution (i.e., the normal distribution):\n",
        "\n",
        "$$ P(x_i|y) = \\frac{1}{\\sqrt {2\\pi\\sigma_y^2}} \\cdot exp(-\\frac{(x_i - \\mu_i)^2}{2\\sigma_y^2}) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQqCIQRKaHyM"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "class NaiveBayes:\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        # calculate mean, var, and prior for each class\n",
        "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._priors =  np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            X_c = X[y==c]\n",
        "            self._mean[idx, :] = X_c.mean(axis=0)\n",
        "            self._var[idx, :] = X_c.var(axis=0)\n",
        "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        # calculate posterior probability for each class\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            prior = np.log(self._priors[idx])\n",
        "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
        "            posterior = prior + posterior\n",
        "            posteriors.append(posterior)\n",
        "            \n",
        "        # return class with highest posterior probability\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "            \n",
        "\n",
        "    # probability density function\n",
        "    # NB; the original code had minor bugs which I fixed below.\n",
        "    #   the original code was\n",
        "    #     numerator = np.exp(- (x-mean)**2 / (2 * var))\n",
        "    #     denominator = np.sqrt(2 * np.pi * var)\n",
        "    #\n",
        "    #   Note that var was not squared.\n",
        "    def _pdf(self, class_idx, x):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = self._var[class_idx]\n",
        "\n",
        "        # This is just an implementation of the normal density function.\n",
        "        # The results are identical with the standard library function\n",
        "        #\n",
        "        numerator = np.exp(- (x-mean)**2 / (2 * var**2))\n",
        "        denominator = np.sqrt(2 * np.pi * var**2)\n",
        "        class_cps = numerator / denominator\n",
        "        #\n",
        "        # class_cps = norm.pdf(x, loc=mean, scale=var)\n",
        "\n",
        "        # print(\"class_conditional probabilities\", class_cps)\n",
        "        # print(\"  norm.pdf\", norm.pdf(x, loc=mean, scale=var))\n",
        "\n",
        "        return class_cps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvZ_-30qlTao"
      },
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJwAAAAkCAYAAAB8IVNVAAAFZ0lEQVR4Ae2aPc7UPBSF7xKgpkGsgJ4CxAY+lkBJBxUFDQg6qm8FgMQCYAewA9gB7AB2AHpeckYnlvMmTpyZZMZXsuz45/r+HN/rzCSi0ZgF7kbEy4j4HhEPxiZf4PjtiHgcEY8i4sMF6l9dZQwKfW2A6yzRrwAahxICcGp3Xa2aa4E9Ao7DcmOuwgPr4JfjSR82alTJAnsDHAAg1a1BRDUHHe1n3Ubev8beF8Nzb4ATANZwEKB61TEmhX7q7rnvEyCusffF8Nwb4AQIHKSLPX0ll3vAxHwofSEQf3jzMqXSTW/VUgvsDXAOEL34fE6MAEgATlo0jf6htc5f81td0QJ7A5wiECYAWKRBAU5Ra8w86AwJmP4G6vy7aa2qaYG9Aw6QARzqqRd76cw6XkAU7bDrmnfEnt9K3nxQrGR+b6ONPKDD/Yj40V2M/ZRvRMSsGEttL5DlmAO2qaDNrZ/cx0alBgdwpWsmC3SEiciO8b0cxdgVdEP2ubKyLreWvqP4k03m5u12wayAni6l4YNc4UCInkfEi8rlbQG/exJkSQ1ocoifwpMot/fUOkXPc5yjCM/dTy8dR9FzyWYAVW88RxG2bVLNAgBNKRQMqL1oA6IPYRpmtAnPnj7TZ20GkBBIEUz3nP81wWq+tmg03QKyKfal1CT5bcivub14Q61yNQJkFABGFFLa5BmlIW93XVeVxnlgLYZBsJ8+qWvrFTszdNUFLz4DGisY6dwJx7pt/fDX0B0bCsQEhzGbggnJI3zMlkMb55RUpELhnFDe98fAmhNmimK5dSV9NyPizg7LLVMSx/r1w8Fh06o1SZP6fY1fIfATAQOf008w+rLGf6Vsoo3RBgDqZA0BTloj1FjKHOMhXkvqhxHxeofliSkN2HQ4AZ/7xKZVaeITfAcBbEUx72d/xlS66csqmKZp8LcJgwCKhLmddDI0lgu7GFLKaZ7XKDuWThlH8XOmsatHLd0B9X9dwab+G2uVu9p1gqZhnGd/K+UZ0DkBUoGQuWoDNrV9vqcJ72/tvgWwZXqocvbsryp7wnf4UwWfyW/slQsYZTv0ZxNo2PNAIFq5nEHavqn6Dgu6sI/AjLngQ8bZKuDeRcTHE5c3ZljARvSBdKix8Z4J3/f8r/sbig0p11vQARLjCLnXrWWMkL01QvbV08cMpZEL2w75YgbLky5R9LwSAuXS+1tOOtLqUPTKzfc+nCpgev+p26eUa4v2WNMfB+yQDr9NPE1zogGnlD22Rjh8jj5L9cDwKunVZSnvra7vRWseeh3XSE1oLEmNpfOv2br6EIfAoww2QDf6uT48rb7jP4bw1v2Yu9oWD2Nt1d3OxbxlrCkLS+ZO4VdrDnJ5dOOZK4MT14xDGvCBhW2/lwG4kgO8cOu2/FQWIKoQ0USAjV/TnQBD+qLk40vbgNyj3VJ+bf1GLYCjuTs5EXU84jGWAo51fPmbK4w55eZ6ZGMc/uk659HaO7IA0YtPwnMOTaPbkFqkVKVZwAJIdb+j1jNtBxNr+JuPfngQxWiLFzJ5GlX/kBytf+MWwKEAAGfj6JTS6JaO8wwgPOI5QNRW7esBj69DjjQtA0Y+dFBZ457oMrX2kSyA84lyTvT53c3H1AYkpLscAQ6tz4GZ/589qtIe+6ght0/r26EFcDYA8JQ1Ft14fR8CGybw6JUCDiCm0QyA+podmrGJXGIBQKEoB/AcfCkfAJqmSX9mXIAFXGqLD30pCAGbIqLmtfqMLQBIuCvloo+rzTx9ZOifSDngAJOeiYTcEZ0ckPQDbs33ea195hYgyvwaiW7c2wBUWjw6MQaoRLlUCR9ARiptLwOy1IXVuWh0YSbYjrp/AYASJpkG18cbAAAAAElFTkSuQmCC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C0aHJYKbqDt"
      },
      "source": [
        "# Test\n",
        "Using the NaiveClass, we can run against the skiing example in 8.4. We can see that it correctly classified all examples except one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK4kVscObsCR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = np.array([\n",
        "      [6., 1.],\n",
        "      [7., 3.],\n",
        "      [8., 2.],    \n",
        "      [9., 0.],\n",
        "      [8., 4.],\n",
        "      [8., 6.],\n",
        "      [9., 2.],    \n",
        "      [9., 5.]\n",
        "    ])\n",
        "y = np.array([0, 0, 0, 0, 1, 1, 1, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HAOxIyWcATh",
        "outputId": "b67f698d-64b8-461f-9f4e-c99697095008"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj3YY2J8bs1b",
        "outputId": "da86a4f4-cbbd-43c6-dc92-01bca8caa6a5"
      },
      "source": [
        "model_nb = NaiveBayes()\n",
        "model_nb.fit(X, y)\n",
        "predictions = model_nb.predict(X)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WuQkfsMcJND",
        "outputId": "f80465c7-bc1d-4fbc-fb86-5f85043497db"
      },
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "    return accuracy\n",
        "\n",
        "accuracy(y, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDlgyRrkgRoK"
      },
      "source": [
        "# Naive Bayes for XOR?\n",
        "Would the naive Bayes model be able to solve the XOR problem?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_guJX9kclgw",
        "outputId": "97887f67-5dcb-4aca-aee3-ec1d83c1d48f"
      },
      "source": [
        "X_xor = np.array([[1., 1.],\n",
        "                  [1., 0.],\n",
        "                  [0., 1.],\n",
        "                  [0., 0.]])\n",
        "y_xor = np.array([0., 1., 1., 0.])\n",
        "\n",
        "model_xor = NaiveBayes()\n",
        "model_xor.fit(X_xor, y_xor)\n",
        "predictions = model_xor.predict(X_xor)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PBeNtsKdfV3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}