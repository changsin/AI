{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Neural Style Transfer with Eager Execution",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/AI/blob/main/10.rl_value_iteration_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS4zwH2SU11P"
      },
      "source": [
        "# Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viHkzdzQh7mk"
      },
      "source": [
        "## Value Iteration\n",
        "\n",
        "In reinforcement learning, to find the optimal policy two methods are used: value iteration or policy iteration. Using the value iteration method, the optimal policy is calculated using Bellman's equation:\n",
        "\n",
        "$$ \\pi^*(s) = \\underset{a} argmax[r(s, a) + \\gamma V^*(\\delta(s, a))] $$\n",
        "\n",
        "- $ $$ \\pi^*(s) $: optimal policy\n",
        "- $ r(s, a) $: immediate reward\n",
        "- $ \\gamma $: discount factor\n",
        "- $ V^*(\\delta(s, a)) $: future reward\n",
        "\n",
        "The optimal policy results in the maximum value $ V^* $:\n",
        "$$ \\hat V(s) = \\underset{a} max[r(s, a) + \\gamma V^*(\\delta(s, a))] $$\n",
        "\n",
        "Using the equation, the state value is updated till there is no more update needed. The following code implements the algorithm using the example in p. 297 of Ertel (2017)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ATn955ngXS_"
      },
      "source": [
        "import numpy as np\n",
        "from enum import Enum\n",
        "\n",
        "class Move(Enum):\n",
        "  L = 1\n",
        "  R = 2\n",
        "  U = 3\n",
        "  D = 4\n",
        "  \n",
        "  def __str__(self):\n",
        "    return str(self.name)\n",
        "\n",
        "class Grid:\n",
        "  # to force convergence, we consider the values are converged\n",
        "  # if the updated value is less than the precision value\n",
        "  PRECISION = 0.001\n",
        "  MOVES = [Move.L, Move.R, Move.U, Move.D]\n",
        "\n",
        "  def __init__(self, state, actions, gamma = 0.9, rows=3, columns=3):\n",
        "    self.state = state\n",
        "    self.actions = actions\n",
        "    self.gamma = gamma\n",
        "    self.rows = rows\n",
        "    self.columns = columns\n",
        "  \n",
        "  def __repr__(self):\n",
        "    rows = \"\"\n",
        "    for row in range(self.rows):\n",
        "      columns = \"\"\n",
        "      for col in range(self.columns):\n",
        "        columns += \" {:03.02f} \".format(self.state[row][col])\n",
        "      rows += \"\\n\" + columns\n",
        "      # print(row_values)\n",
        "    return rows\n",
        "\n",
        "  def converge(self, limit=100):\n",
        "    for i in range(limit):\n",
        "      if self.update_state() == 0:\n",
        "        print(\"Converged at \", i)\n",
        "        return\n",
        "    \n",
        "    print(\"Did not converge after \", limit)\n",
        "\n",
        "  def update_state(self):\n",
        "    updated = 0\n",
        "    # update the state from bottom to top, left to right\n",
        "    for row in range(self.rows - 1, -1, -1):\n",
        "      for col in range(0, self.columns, 1):\n",
        "        value_original = self.state[row][col]\n",
        "        self.state[row][col] = self.calc_optimal_value((row, col))\n",
        "\n",
        "        if abs(value_original - self.state[row][col]) > self.PRECISION:\n",
        "          updated += 1\n",
        "    print(self)\n",
        "    return updated\n",
        "  \n",
        "  def to_target_pos(self, pos, move):\n",
        "    cur_row, cur_col = pos\n",
        "    if Move.L == move:\n",
        "      return (cur_row, cur_col - 1)\n",
        "    elif Move.R == move:\n",
        "      return (cur_row, cur_col + 1)\n",
        "    elif Move.U == move:\n",
        "      return (cur_row - 1, cur_col)\n",
        "    elif Move.D == move:\n",
        "      return (cur_row + 1, cur_col)\n",
        "    else:\n",
        "      raise Exception(\"Invalid move\")\n",
        "\n",
        "  def is_valid_move(self, pos, move):\n",
        "    target_row, target_col = self.to_target_pos(pos, move)\n",
        "    return target_row < self.rows and target_col < self.columns and \\\n",
        "      target_row >= 0 and target_col >= 0\n",
        "\n",
        "  def get_immediate_reward(self, pos, move):\n",
        "    return self.actions[pos, move] if (pos, move) in self.actions else 0\n",
        "\n",
        "  def get_state_value(self, pos):\n",
        "    return self.state[pos[0]][pos[1]]\n",
        "\n",
        "  def calc_optimal_value(self, pos):\n",
        "    \"\"\"\n",
        "    pi*(s) = argmax[r(s, a) + \\gamma V*(\\delta(s, a))]\n",
        "    \"\"\"\n",
        "    possible_values = []\n",
        "    for move in self.MOVES:\n",
        "      if self.is_valid_move(pos, move):\n",
        "        target_pos = self.to_target_pos(pos, move)\n",
        "        value = self.get_immediate_reward(pos, move) + self.gamma*self.get_state_value(target_pos)\n",
        "        possible_values.append(value)\n",
        "    return max(possible_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yeptpjzgr53"
      },
      "source": [
        "With the class defined, let's now run to see how it converges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl3FRHfQgqrB"
      },
      "source": [
        "grid_state = [[0, 0, 0],\n",
        "              [0, 0, 0],\n",
        "              [0, 0, 0]]\n",
        "\n",
        "actions = {}\n",
        "actions[(2, 0), Move.U] = -1\n",
        "actions[(2, 0), Move.R] = -1\n",
        "actions[(2, 1), Move.R] = -1\n",
        "actions[(2, 1), Move.L] = 1\n",
        "actions[(2, 2), Move.L] = 1\n",
        "\n",
        "\n",
        "grid = Grid(grid_state, actions)\n",
        "# print(grid)\n",
        "# grid.update_state()\n",
        "grid.converge()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psDm8jt-g1lw"
      },
      "source": [
        "We can see that it converges after 33 steps. In reality, it keeps improving, but with ever smaller vanishing floating points so it is better to set the minimum threshold where we say that it is good enough. In our case, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh_VM78RL28a"
      },
      "source": [
        "## Q Learning\n",
        "\n",
        "- Model-free: Q Learning is a model free reinforcement learning algorithm. \"Model-free\" means that you do not need to have the complete knowledge about the world to learn the algorithm. In constrast, the value iteration method outlined above requies the model of the world.\n",
        "\n",
        "- Q learning find the optimal policy that the agent learned over time. By \"quality\", it refers to the quality of the actions from a state.\n",
        "\n",
        "- The agent learns the optimal policy by maximizing the expected Q value of the total reward for all the states, starting from the current state.\n",
        "\n",
        "- Q value is calculate using the following formula:\n",
        "\n",
        "\n",
        "$$ \\hat Q(s, a) = r(s, a) + \\gamma \\space \\underset{a'}max \\space \\hat Q(\\delta(s, a), a') $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32rs4Geqh6pG"
      },
      "source": [
        "import numpy as np\n",
        "from enum import Enum\n",
        "\n",
        "def print_actions(actions):\n",
        "  for action, reward in actions.items():\n",
        "    print(\"{} {} {}\".format(action[0], action[1], reward))\n",
        "\n",
        "class GridQ:\n",
        "  # to force convergence, we consider the values are converged\n",
        "  # if the updated value is less than the precision value\n",
        "  PRECISION = 0.001\n",
        "  MOVES = [Move.L, Move.R, Move.U, Move.D]\n",
        "\n",
        "  def __init__(self, state, actions,\n",
        "               gamma = 0.9, rows=3, columns=3, q_actions=None):\n",
        "    self.state = state\n",
        "    self.actions = actions\n",
        "    self.q_actions = q_actions\n",
        "\n",
        "    self.gamma = gamma\n",
        "    self.rows = rows\n",
        "    self.columns = columns\n",
        "\n",
        "    self._init_q_actions()\n",
        "  \n",
        "  def _init_q_actions(self):\n",
        "    if not self.q_actions:\n",
        "      self.q_actions = {}\n",
        "\n",
        "    for row in range(self.rows):\n",
        "      for col in range(self.columns):\n",
        "        for move in self.MOVES:\n",
        "          pos = row, col\n",
        "          if self.is_valid_move(pos, move):\n",
        "            if (pos, move) not in self.q_actions:\n",
        "              self.q_actions[(row, col), move] = 0\n",
        "\n",
        "    # print_actions(self.q_actions)\n",
        "\n",
        "  def __repr__(self):\n",
        "    rows = \"\"\n",
        "    for row in range(self.rows):\n",
        "      columns = \"\"\n",
        "      for col in range(self.columns):\n",
        "        columns += \" {:03.02f} \".format(self.state[row][col])\n",
        "      rows += \"\\n\" + columns\n",
        "      # print(row_values)\n",
        "    return rows\n",
        "\n",
        "  def converge(self, limit=100):\n",
        "    for i in range(limit):\n",
        "      if self.update_state() == 0:\n",
        "        print(\"Converged at \", i)\n",
        "        return\n",
        "    \n",
        "    print(\"Did not converge after \", limit)\n",
        "\n",
        "  def update_state(self):\n",
        "    updated = 0\n",
        "    # update the state from bottom to top, left to right\n",
        "    for row in range(self.rows - 1, -1, -1):\n",
        "      for col in range(0, self.columns, 1):\n",
        "        value_original = self.state[row][col]\n",
        "        self.state[row][col] = self.get_optimal_value((row, col))\n",
        "\n",
        "        if abs(value_original - self.state[row][col]) > self.PRECISION:\n",
        "          updated += 1\n",
        "    print(self)\n",
        "    return updated\n",
        "  \n",
        "  def to_target_pos(self, pos, move):\n",
        "    cur_row, cur_col = pos\n",
        "    if Move.L == move:\n",
        "      return (cur_row, cur_col - 1)\n",
        "    elif Move.R == move:\n",
        "      return (cur_row, cur_col + 1)\n",
        "    elif Move.U == move:\n",
        "      return (cur_row - 1, cur_col)\n",
        "    elif Move.D == move:\n",
        "      return (cur_row + 1, cur_col)\n",
        "    else:\n",
        "      raise Exception(\"Invalid move\")\n",
        "\n",
        "  def is_valid_move(self, pos, move):\n",
        "    target_row, target_col = self.to_target_pos(pos, move)\n",
        "    return target_row < self.rows and target_col < self.columns and \\\n",
        "      target_row >= 0 and target_col >= 0\n",
        "\n",
        "  def get_action_reward(self, pos, move):\n",
        "    return self.actions[pos, move] if (pos, move) in self.actions else 0\n",
        "\n",
        "  def get_state_value(self, pos):\n",
        "    return self.state[pos[0]][pos[1]]\n",
        "\n",
        "  def get_optimal_value(self, pos):\n",
        "    \"\"\"\n",
        "    pi*(s) = argmax[r(s, a) + \\gamma V*(\\delta(s, a))]\n",
        "    \"\"\"\n",
        "    possible_values = []\n",
        "    for move in self.MOVES:\n",
        "      if self.is_valid_move(pos, move):\n",
        "        target_pos = self.to_target_pos(pos, move)\n",
        "        value = self.get_action_reward(pos, move) \\\n",
        "          + self.gamma*self.get_state_value(target_pos)\n",
        "        possible_values.append(value)\n",
        "    return max(possible_values)\n",
        "\n",
        "  def get_max_q_value(self, pos, move):\n",
        "    def _max_possible_q_value(pos):\n",
        "      q_values = []\n",
        "      for move in self.MOVES:\n",
        "        if self.is_valid_move(pos, move):\n",
        "          print(pos, move)\n",
        "          q_values.append(self.q_actions[pos, move])\n",
        "\n",
        "      return max(q_values)    \n",
        "\n",
        "    \"\"\"\n",
        "    pi*(s) = argmax[r(s, a) + \\gamma V*(\\delta(s, a))]\n",
        "    \"\"\"\n",
        "    pos_next = self.to_target_pos(pos, move)\n",
        "    q_value = self.get_action_reward(pos, move) \\\n",
        "      + self.gamma*_max_possible_q_value(pos_next)\n",
        "    return q_value\n",
        "\n",
        "  def calc_q_value(self, pos, move, next_pos, next_move):\n",
        "    q_value_cur = self.get_action_reward(pos, move)\n",
        "    q_value_updated = self.get_max_q_value(pos, move, next_pos, next_move)\n",
        "\n",
        "    print(\"Q-value: {} Q-value-updated: {}\".format(q_value_cur,\n",
        "                                                   q_value_updated))\n",
        "    return q_value_updated if q_value_cur != q_value_updated else q_value_cur\n",
        "\n",
        "  def take_actions(self, actions):\n",
        "    \"\"\"\n",
        "    take the number of actions and calculate Q values\n",
        "    \"\"\"\n",
        "    # print_actions(self.q_actions)\n",
        "    for id in range(len(actions)):\n",
        "      pos, move = actions[id]\n",
        "      q_value = self.get_max_q_value(pos, move)\n",
        "      print(\"q_value is {} for {} {}\".format(q_value, pos, move))\n",
        "\n",
        "      self.q_actions[(pos, move)] = q_value\n",
        "    \n",
        "    # print_actions(self.q_actions)\n",
        "  \n",
        "  def build_actions(self, pos_start, moves):\n",
        "    actions = []\n",
        "    pos = pos_start\n",
        "\n",
        "    for move in moves:\n",
        "      if self.is_valid_move(pos, move):\n",
        "        actions.append((pos, move))\n",
        "        pos = self.to_target_pos(pos, move)\n",
        "\n",
        "    return actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0vqmuNih4oM"
      },
      "source": [
        "### Q Learning in action\n",
        "The newly defined class GridQ still works for the value iteration. Below is the proof."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sylWs2uCRXyO"
      },
      "source": [
        "state_q = [[0, 0, 0],\n",
        "          [0, 0, 0],\n",
        "          [0, 0, 0]]\n",
        "\n",
        "actions = {}\n",
        "actions[(2, 0), Move.R] = -1\n",
        "actions[(2, 1), Move.R] = -1\n",
        "actions[(2, 1), Move.L] = 1\n",
        "actions[(2, 2), Move.L] = 1\n",
        "\n",
        "grid_q = Grid(state_q, actions)\n",
        "grid_q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcwQJHbcD-py"
      },
      "source": [
        "grid_state = [[0, 0, 0],\n",
        "              [0, 0, 0],\n",
        "              [0, 0, 0]]\n",
        "\n",
        "actions = {}\n",
        "actions[(2, 0), Move.R] = -1\n",
        "actions[(2, 1), Move.R] = -1\n",
        "actions[(2, 1), Move.L] = 1\n",
        "actions[(2, 2), Move.L] = 1\n",
        "\n",
        "\n",
        "gridq = GridQ(grid_state, actions)\n",
        "# print(grid)\n",
        "# grid.update_state()\n",
        "gridq.converge()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf0q15oRims_"
      },
      "source": [
        "Using the Q Learning formula, we can now start exploring to get Q values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3LLX0o5r7Hr"
      },
      "source": [
        "walk1 = grid_q.build_actions((2, 0),\n",
        "                             [Move.R, Move.R, Move.L, Move.L])\n",
        "grid_q.take_actions(walk1)\n",
        "print_actions(grid_q.q_actions)\n",
        "\n",
        "walk2 = grid_q.build_actions((1, 1),\n",
        "                             [Move.D, Move.L, Move.U, Move.R])\n",
        "grid_q.take_actions(walk2)\n",
        "print_actions(grid_q.q_actions)\n",
        "\n",
        "walk3 = grid_q.build_actions((1, 0),\n",
        "                             [Move.D, Move.R, Move.R, Move.U, Move.L, Move.L])\n",
        "grid_q.take_actions(walk3)\n",
        "print_actions(grid_q.q_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSWZJs3Ol2Qs"
      },
      "source": [
        "## Q Learning in Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nQ4XLhHl1Zk"
      },
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWlOP5eul8t6"
      },
      "source": [
        "#Fixing seed for reproducibility\n",
        "np.random.seed(0)\n",
        "#Loading and rendering the gym environment\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaqMHB9TmMFj"
      },
      "source": [
        "#Getting the state space\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))\n",
        "\n",
        "#STEP 1 - Initializing the Q-table\n",
        "\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "#Setting the hyperparameters\n",
        "              \n",
        "alpha = 0.7 #learning rate                 \n",
        "discount_factor = 0.618               \n",
        "epsilon = 1                  \n",
        "max_epsilon = 1\n",
        "min_epsilon = 0.01         \n",
        "decay = 0.01         \n",
        "\n",
        "train_episodes = 2000    \n",
        "test_episodes = 100          \n",
        "max_steps = 100 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H49xq-j2miGL"
      },
      "source": [
        "#Training the agent\n",
        "\n",
        "#Creating lists to keep track of reward and epsilon values\n",
        "training_rewards = []  \n",
        "epsilons = []\n",
        "\n",
        "for episode in range(train_episodes):\n",
        "    #Reseting the environment each time as per requirement\n",
        "    state = env.reset()    \n",
        "    #Starting the tracker for the rewards\n",
        "    total_training_rewards = 0\n",
        "    \n",
        "    for step in range(100):\n",
        "        #Choosing an action given the states based on a random number\n",
        "        exp_exp_tradeoff = random.uniform(0, 1) \n",
        "        \n",
        "        \n",
        "        ### STEP 2: SECOND option for choosing the initial action - exploit     \n",
        "        #If the random number is larger than epsilon: employing exploitation \n",
        "        #and selecting best action \n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(Q[state,:])      \n",
        "            \n",
        "        ### STEP 2: FIRST option for choosing the initial action - explore       \n",
        "        #Otherwise, employing exploration: choosing a random action \n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "            \n",
        "            \n",
        "        ### STEPs 3 & 4: performing the action and getting the reward     \n",
        "        #Taking the action and getting the reward and outcome state\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "\n",
        "        ### STEP 5: update the Q-table\n",
        "        #Updating the Q-table using the Bellman equation\n",
        "        Q[state, action] = Q[state, action] + alpha * (reward + discount_factor * np.max(Q[new_state, :]) - Q[state, action]) \n",
        "        #Increasing our total reward and updating the state\n",
        "        total_training_rewards += reward      \n",
        "        state = new_state         \n",
        "        \n",
        "        #Ending the episode\n",
        "        if done == True:\n",
        "            #print (\"Total reward for episode {}: {}\".format(episode, total_training_rewards))\n",
        "            break\n",
        "    \n",
        "    #Cutting down on exploration by reducing the epsilon \n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay*episode)\n",
        "    \n",
        "    #Adding the total reward and reduced epsilon values\n",
        "    training_rewards.append(total_training_rewards)\n",
        "    epsilons.append(epsilon)\n",
        "\n",
        "print (\"Training score over time: \" + str(sum(training_rewards)/train_episodes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNP4BSbZnV1l"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqpvroKLmp2k"
      },
      "source": [
        "#Visualizing results and total reward over all episodes\n",
        "x = range(train_episodes)\n",
        "plt.plot(x, training_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Training total reward')\n",
        "plt.title('Total rewards over all episodes in training') \n",
        "plt.show()\n",
        "\n",
        "#Visualizing the epsilons over all episodes\n",
        "plt.plot(epsilons)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Epsilon')\n",
        "plt.title(\"Epsilon for episode\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ssKoHgvSeBj"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "#create environment\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "#initialize Q-table\n",
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "gamma = 0.1\n",
        "alpha = 0.1\n",
        "epsilon = 0.1\n",
        "epsilon_decay = 0.99 #decay factor \n",
        "\n",
        "\n",
        "total_epochs = 0\n",
        "episodes = 10000\n",
        "\n",
        "for episode in range(episodes):\n",
        "    epochs = 0\n",
        "    reward = 0\n",
        "    epsilon = epsilon * epsilon_decay #decay step\n",
        "\n",
        "    state = env.reset()\n",
        "    \n",
        "    while reward != 20:\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q[state])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * \\\n",
        "                            np.max(Q[next_state]) - Q[state, action])\n",
        "        state = next_state \n",
        "        epochs += 1\n",
        "    total_epochs += epochs\n",
        "    \n",
        "print(\"Average timesteps taken: {}\".format(total_epochs/episodes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wiQA9RbizO3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}