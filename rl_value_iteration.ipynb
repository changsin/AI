{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Neural Style Transfer with Eager Execution",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/AI/blob/main/rl_value_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS4zwH2SU11P"
      },
      "source": [
        "# Value Iteration\n",
        "\n",
        "In reinforcement learning, to find the optimal policy two methods are used: value iteration or policy iteration. Using the value iteration method, the optimal policy is calculated using Bellman's equation:\n",
        "\n",
        "$$ \\pi^*(s) = \\underset{a} argmax[r(s, a) + \\gamma V^*(\\delta(s, a))] $$\n",
        "\n",
        "- $ $$ \\pi^*(s) $: optimal policy\n",
        "- $ r(s, a) $: immediate reward\n",
        "- $ \\gamma V^*(\\delta(s, a)) $: \n",
        "\n",
        "The optimal policy results in the maximum value $ V^* $:\n",
        "$$ \\hat V(s) = \\underset{a} max[r(s, a) + \\gamma V^*(\\delta(s, a))] $$\n",
        "\n",
        "Using the equation, the state value is updated till there is no more update needed. The following code implements the algorithm using the example in p. 297 of Ertel (2017)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32rs4Geqh6pG"
      },
      "source": [
        "import numpy as np\n",
        "from enum import Enum\n",
        "\n",
        "class Action:\n",
        "  def __init__(from_pos, move, reward):\n",
        "    this.from_pos = from_pos\n",
        "    this.move = move\n",
        "    this.reward = reward\n",
        "\n",
        "class Move(Enum):\n",
        "  L = 1\n",
        "  R = 2\n",
        "  U = 3\n",
        "  D = 4\n",
        "\n",
        "class Grid:\n",
        "  # to force convergence, we consider the values are converged\n",
        "  # if the updated value is less than the precision value\n",
        "  PRECISION = 0.001\n",
        "  MOVES = [Move.L, Move.R, Move.U, Move.D]\n",
        "\n",
        "  def __init__(self, state, actions, gamma = 0.9, rows=3, columns=3):\n",
        "    self.state = state\n",
        "    self.actions = actions\n",
        "    self.gamma = gamma\n",
        "    self.rows = rows\n",
        "    self.columns = columns\n",
        "  \n",
        "  def __repr__(self):\n",
        "    rows = \"\"\n",
        "    for row in range(self.rows):\n",
        "      columns = \"\"\n",
        "      for col in range(self.columns):\n",
        "        columns += \" {:03.02f} \".format(self.state[row][col])\n",
        "      rows += \"\\n\" + columns\n",
        "      # print(row_values)\n",
        "    return rows\n",
        "\n",
        "  def converge(self, limit=100):\n",
        "    for i in range(limit):\n",
        "      if self.update_state() == 0:\n",
        "        print(\"Converged at \", i)\n",
        "        return\n",
        "    \n",
        "    print(\"Did not converge after \", limit)\n",
        "\n",
        "  def update_state(self):\n",
        "    updated = 0\n",
        "    # update the state from bottom to top, left to right\n",
        "    for row in range(self.rows - 1, -1, -1):\n",
        "      for col in range(0, self.columns, 1):\n",
        "        value_original = self.state[row][col]\n",
        "        self.state[row][col] = self.calc_optimal_value((row, col))\n",
        "\n",
        "        if abs(value_original - self.state[row][col]) > self.PRECISION:\n",
        "          updated += 1\n",
        "    print(self)\n",
        "    return updated\n",
        "  \n",
        "  def to_target_pos(self, pos, move):\n",
        "    cur_row, cur_col = pos\n",
        "    if Move.L == move:\n",
        "      return (cur_row, cur_col - 1)\n",
        "    elif Move.R == move:\n",
        "      return (cur_row, cur_col + 1)\n",
        "    elif Move.U == move:\n",
        "      return (cur_row - 1, cur_col)\n",
        "    elif Move.D == move:\n",
        "      return (cur_row + 1, cur_col)\n",
        "    else:\n",
        "      raise Exception(\"Invalid move\")\n",
        "\n",
        "  def is_valid_move(self, pos, move):\n",
        "    target_row, target_col = self.to_target_pos(pos, move)\n",
        "    return target_row < self.rows and target_col < self.columns and \\\n",
        "      target_row >= 0 and target_col >= 0\n",
        "\n",
        "  def get_immediate_reward(self, pos, move):\n",
        "    return self.actions[pos, move] if (pos, move) in self.actions else 0\n",
        "\n",
        "  def get_state_value(self, pos):\n",
        "    return self.state[pos[0]][pos[1]]\n",
        "\n",
        "  def calc_optimal_value(self, pos):\n",
        "    \"\"\"\n",
        "    pi*(s) = argmax[r(s, a) + \\gamma V*(\\delta(s, a))]\n",
        "    \"\"\"\n",
        "    possible_values = []\n",
        "    for move in self.MOVES:\n",
        "      if self.is_valid_move(pos, move):\n",
        "        target_pos = self.to_target_pos(pos, move)\n",
        "        value = self.get_immediate_reward(pos, move) + self.gamma*self.get_state_value(target_pos)\n",
        "        possible_values.append(value)\n",
        "    return max(possible_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcwQJHbcD-py"
      },
      "source": [
        "grid_state = [[0, 0, 0],\n",
        "              [0, 0, 0],\n",
        "              [0, 0, 0]]\n",
        "\n",
        "actions = {}\n",
        "actions[(2, 0), Move.R] = -1\n",
        "actions[(2, 1), Move.R] = -1\n",
        "actions[(2, 1), Move.L] = 1\n",
        "actions[(2, 2), Move.L] = 1\n",
        "\n",
        "\n",
        "grid = Grid(grid_state, actions)\n",
        "# print(grid)\n",
        "# grid.update_state()\n",
        "grid.converge()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ssKoHgvSeBj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}