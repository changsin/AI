{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Neural Style Transfer with Eager Execution",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/AI/blob/main/rl_value_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS4zwH2SU11P"
      },
      "source": [
        "# Value Iteration\n",
        "\n",
        "In reinforcement learning, to find the optimal policy two methods are used: value iteration or policy iteration. Using the value iteration method, the optimal policy is calculated using Bellman's equation:\n",
        "\n",
        "$$ \\pi^*(s) = \\underset{a} argmax[r(s, a) + \\gamma V^*(\\delta(s, a))] $$\n",
        "\n",
        "- $ $$ \\pi^*(s) $: optimal policy\n",
        "- $ r(s, a) $: immediate reward\n",
        "- $ \\gamma $: discount factor\n",
        "- $ V^*(\\delta(s, a)) $: future reward\n",
        "\n",
        "The optimal policy results in the maximum value $ V^* $:\n",
        "$$ \\hat V(s) = \\underset{a} max[r(s, a) + \\gamma V^*(\\delta(s, a))] $$\n",
        "\n",
        "Using the equation, the state value is updated till there is no more update needed. The following code implements the algorithm using the example in p. 297 of Ertel (2017)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32rs4Geqh6pG"
      },
      "source": [
        "import numpy as np\n",
        "from enum import Enum\n",
        "\n",
        "class Action:\n",
        "  def __init__(from_pos, move, reward):\n",
        "    this.from_pos = from_pos\n",
        "    this.move = move\n",
        "    this.reward = reward\n",
        "\n",
        "class Move(Enum):\n",
        "  L = 1\n",
        "  R = 2\n",
        "  U = 3\n",
        "  D = 4\n",
        "\n",
        "class Grid:\n",
        "  # to force convergence, we consider the values are converged\n",
        "  # if the updated value is less than the precision value\n",
        "  PRECISION = 0.001\n",
        "  MOVES = [Move.L, Move.R, Move.U, Move.D]\n",
        "\n",
        "  def __init__(self, state, actions, gamma = 0.9, rows=3, columns=3):\n",
        "    self.state = state\n",
        "    self.actions = actions\n",
        "    self.gamma = gamma\n",
        "    self.rows = rows\n",
        "    self.columns = columns\n",
        "  \n",
        "  def __repr__(self):\n",
        "    rows = \"\"\n",
        "    for row in range(self.rows):\n",
        "      columns = \"\"\n",
        "      for col in range(self.columns):\n",
        "        columns += \" {:03.02f} \".format(self.state[row][col])\n",
        "      rows += \"\\n\" + columns\n",
        "      # print(row_values)\n",
        "    return rows\n",
        "\n",
        "  def converge(self, limit=100):\n",
        "    for i in range(limit):\n",
        "      if self.update_state() == 0:\n",
        "        print(\"Converged at \", i)\n",
        "        return\n",
        "    \n",
        "    print(\"Did not converge after \", limit)\n",
        "\n",
        "  def update_state(self):\n",
        "    updated = 0\n",
        "    # update the state from bottom to top, left to right\n",
        "    for row in range(self.rows - 1, -1, -1):\n",
        "      for col in range(0, self.columns, 1):\n",
        "        value_original = self.state[row][col]\n",
        "        self.state[row][col] = self.calc_optimal_value((row, col))\n",
        "\n",
        "        if abs(value_original - self.state[row][col]) > self.PRECISION:\n",
        "          updated += 1\n",
        "    print(self)\n",
        "    return updated\n",
        "  \n",
        "  def to_target_pos(self, pos, move):\n",
        "    cur_row, cur_col = pos\n",
        "    if Move.L == move:\n",
        "      return (cur_row, cur_col - 1)\n",
        "    elif Move.R == move:\n",
        "      return (cur_row, cur_col + 1)\n",
        "    elif Move.U == move:\n",
        "      return (cur_row - 1, cur_col)\n",
        "    elif Move.D == move:\n",
        "      return (cur_row + 1, cur_col)\n",
        "    else:\n",
        "      raise Exception(\"Invalid move\")\n",
        "\n",
        "  def is_valid_move(self, pos, move):\n",
        "    target_row, target_col = self.to_target_pos(pos, move)\n",
        "    return target_row < self.rows and target_col < self.columns and \\\n",
        "      target_row >= 0 and target_col >= 0\n",
        "\n",
        "  def get_immediate_reward(self, pos, move):\n",
        "    return self.actions[pos, move] if (pos, move) in self.actions else 0\n",
        "\n",
        "  def get_state_value(self, pos):\n",
        "    return self.state[pos[0]][pos[1]]\n",
        "\n",
        "  def calc_optimal_value(self, pos):\n",
        "    \"\"\"\n",
        "    pi*(s) = argmax[r(s, a) + \\gamma V*(\\delta(s, a))]\n",
        "    \"\"\"\n",
        "    possible_values = []\n",
        "    for move in self.MOVES:\n",
        "      if self.is_valid_move(pos, move):\n",
        "        target_pos = self.to_target_pos(pos, move)\n",
        "        value = self.get_immediate_reward(pos, move) + self.gamma*self.get_state_value(target_pos)\n",
        "        possible_values.append(value)\n",
        "    return max(possible_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcwQJHbcD-py"
      },
      "source": [
        "grid_state = [[0, 0, 0],\n",
        "              [0, 0, 0],\n",
        "              [0, 0, 0]]\n",
        "\n",
        "actions = {}\n",
        "actions[(2, 0), Move.U] = -1\n",
        "actions[(2, 0), Move.R] = -1\n",
        "actions[(2, 1), Move.R] = -1\n",
        "actions[(2, 1), Move.L] = 1\n",
        "actions[(2, 2), Move.L] = 1\n",
        "\n",
        "\n",
        "grid = Grid(grid_state, actions)\n",
        "# print(grid)\n",
        "# grid.update_state()\n",
        "grid.converge()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENsx2WhW3yNx"
      },
      "source": [
        "grid_state1 = [[0, 0, 0],\n",
        "              [0, 0, 0],\n",
        "              [0, 0, 0]]\n",
        "\n",
        "actions1 = {}\n",
        "actions1[(1, 1), Move.R] = 40\n",
        "actions1[(2, 2), Move.R] = 100\n",
        "actions1[(0, 2), Move.L] = 70\n",
        "\n",
        "\n",
        "grid1 = Grid(grid_state1, actions1)\n",
        "# print(grid)\n",
        "# grid.update_state()\n",
        "grid1.converge()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3tYI1Uj3pIe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSWZJs3Ol2Qs"
      },
      "source": [
        "# Q Learning in Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nQ4XLhHl1Zk"
      },
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWlOP5eul8t6"
      },
      "source": [
        "#Fixing seed for reproducibility\n",
        "np.random.seed(0)\n",
        "#Loading and rendering the gym environment\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaqMHB9TmMFj"
      },
      "source": [
        "#Getting the state space\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))\n",
        "\n",
        "#STEP 1 - Initializing the Q-table\n",
        "\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "#Setting the hyperparameters\n",
        "              \n",
        "alpha = 0.7 #learning rate                 \n",
        "discount_factor = 0.618               \n",
        "epsilon = 1                  \n",
        "max_epsilon = 1\n",
        "min_epsilon = 0.01         \n",
        "decay = 0.01         \n",
        "\n",
        "train_episodes = 2000    \n",
        "test_episodes = 100          \n",
        "max_steps = 100 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H49xq-j2miGL"
      },
      "source": [
        "#Training the agent\n",
        "\n",
        "#Creating lists to keep track of reward and epsilon values\n",
        "training_rewards = []  \n",
        "epsilons = []\n",
        "\n",
        "for episode in range(train_episodes):\n",
        "    #Reseting the environment each time as per requirement\n",
        "    state = env.reset()    \n",
        "    #Starting the tracker for the rewards\n",
        "    total_training_rewards = 0\n",
        "    \n",
        "    for step in range(100):\n",
        "        #Choosing an action given the states based on a random number\n",
        "        exp_exp_tradeoff = random.uniform(0, 1) \n",
        "        \n",
        "        \n",
        "        ### STEP 2: SECOND option for choosing the initial action - exploit     \n",
        "        #If the random number is larger than epsilon: employing exploitation \n",
        "        #and selecting best action \n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(Q[state,:])      \n",
        "            \n",
        "        ### STEP 2: FIRST option for choosing the initial action - explore       \n",
        "        #Otherwise, employing exploration: choosing a random action \n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "            \n",
        "            \n",
        "        ### STEPs 3 & 4: performing the action and getting the reward     \n",
        "        #Taking the action and getting the reward and outcome state\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "\n",
        "        ### STEP 5: update the Q-table\n",
        "        #Updating the Q-table using the Bellman equation\n",
        "        Q[state, action] = Q[state, action] + alpha * (reward + discount_factor * np.max(Q[new_state, :]) - Q[state, action]) \n",
        "        #Increasing our total reward and updating the state\n",
        "        total_training_rewards += reward      \n",
        "        state = new_state         \n",
        "        \n",
        "        #Ending the episode\n",
        "        if done == True:\n",
        "            #print (\"Total reward for episode {}: {}\".format(episode, total_training_rewards))\n",
        "            break\n",
        "    \n",
        "    #Cutting down on exploration by reducing the epsilon \n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay*episode)\n",
        "    \n",
        "    #Adding the total reward and reduced epsilon values\n",
        "    training_rewards.append(total_training_rewards)\n",
        "    epsilons.append(epsilon)\n",
        "\n",
        "print (\"Training score over time: \" + str(sum(training_rewards)/train_episodes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNP4BSbZnV1l"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqpvroKLmp2k"
      },
      "source": [
        "#Visualizing results and total reward over all episodes\n",
        "x = range(train_episodes)\n",
        "plt.plot(x, training_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Training total reward')\n",
        "plt.title('Total rewards over all episodes in training') \n",
        "plt.show()\n",
        "\n",
        "#Visualizing the epsilons over all episodes\n",
        "plt.plot(epsilons)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Epsilon')\n",
        "plt.title(\"Epsilon for episode\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ssKoHgvSeBj"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "#create environment\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "#initialize Q-table\n",
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "gamma = 0.1\n",
        "alpha = 0.1\n",
        "epsilon = 0.1\n",
        "epsilon_decay = 0.99 #decay factor \n",
        "\n",
        "\n",
        "total_epochs = 0\n",
        "episodes = 10000\n",
        "\n",
        "for episode in range(episodes):\n",
        "    epochs = 0\n",
        "    reward = 0\n",
        "    epsilon = epsilon * epsilon_decay #decay step\n",
        "\n",
        "    state = env.reset()\n",
        "    \n",
        "    while reward != 20:\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q[state])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * \\\n",
        "                            np.max(Q[next_state]) - Q[state, action])\n",
        "        state = next_state \n",
        "        epochs += 1\n",
        "    total_epochs += epochs\n",
        "    \n",
        "print(\"Average timesteps taken: {}\".format(total_epochs/episodes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wiQA9RbizO3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}